<!DOCTYPE html>
<html>
    <head>

        <title>
            Steve's Portfolio
        </title>

        <link rel="stylesheet" href="style.css">

    </head>

    <body>
        <div id="container">

            <header>
                <h1>
                    Portfolio 
                </h1>
            </header>

            <div class="group">

                <aside>
                    <a href="index.html">
                        <img alt="" class="logo" src="img/sitelogo.png"/>
                    </a>
                    <a href="index.html">
                        <h1 class = "myname">Steve Curtis</h1>
                    </a>
                    <ul class="menu">
                        <li>
                            <a href="about.html">About.</a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/stevecurtisdev">LinkedIn.</a> 
                        </li>
                    </ul>
                </aside>

                <div class="wrapper">
                    <img alt="" class="bannerimg" src="img/iconIntelligentSensors.png"/>

                    <h1>
                        Intelligent Sensors//////////////
                    </h1>
                    <h2>
                        Prototype
                    </h2>
                    <iframe title="vimeo-player" src="https://player.vimeo.com/video/241547545" width="640" height="360" frameborder="0" allowfullscreen></iframe>
                    <p>
                        This experiment was centered around Wekinator, an open-source software designed to make machine learning accessible to artists and creatives. Inputs/outputs are processed according to a chosen algorithm and the machine is ‘taught’ specific gestures, positions and actions. These are communicated via OSC making it easy to link different projects and interactives together. For this specific work, I utilised Skeletal-input from the Kinect to drive Generative Art sketches within Processing, using machine learning to speed up the process of development.               
                    </p>
                    <iframe title="vimeo-player" src="https://player.vimeo.com/video/268189355" width="640" height="360" frameborder="0" allowfullscreen></iframe>
                    <h1>
                        Intelligent Sensors//////////////
                    </h1>
                    <h2>
                        StageII_Refinement
                    </h2>
                    <p>
                        Stemming from the research so far, I was inspirted to use this project to consider a scenario in which a machine is learning the context of the human form through detected poses.                    
                    </p>

                    <p>
                        Machine learning in computer vision relies upon pattern recognition without any underlying context. What if a machine was trying to learn more about humanity through this process? As mentioned before, human beings would be the primary learning source of a machine deciphering its origins, purpose and how it integrates into society.                    
                    </p>

                    <p>
                        The internet is the largest data footprint mankind has created and contains every depiction possible of humanity. If a machine like searching through a database like Leeloo, what would it make of humanity?                    
                    </p>

                </div>
            </div>



            <footer>
                © Stephen Curtis 2018
            </footer>

        </div>
    </body>

</html>
