<!DOCTYPE html>
<html>
    <head>

        <title>
            Steve's Portfolio
        </title>

        <link rel="stylesheet" href="style.css">

    </head>

    <body>
        <div id="container">

            <header>
                <h1>
                    Portfolio 
                </h1>
            </header>

            <div class="group">

                <aside>
                    <a href="index.html">
                        <img alt="" class="logo" src="img/sitelogo.png"/>
                    </a>
                    <a href="index.html">
                        <h1 class = "myname">Steve Curtis</h1>
                    </a>
                    <ul class="menu">
                        <li>
                            <a href="about.html">About.</a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/stevecurtisdev">LinkedIn.</a> 
                        </li>
                    </ul>
                </aside>

                <div class="wrapper">
                    <h1>
                        MOBILE_Platforms//AndroidCamExperiments
                    </h1>
                    <p>
                        I developed these experiments with the advantages of using mobile platforms in mind (such as mobility, front/back cameras and accelerometers). The aim for both interactives was to utilise the built-in camera to augment an existing sculpture or scene.
                    </p>

                    <img alt="" class="bannerimg" src="img/iconAndroidCam.png"/>

                    <h2>
                        LookingGlass////////PROCESSING
                    </h2>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/JYH8IBR__iY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p>
                        Developed using the processing environment, this interactive work randomly generates a primitive sculpture upon execution. The context of the object is strongly altered by using the camera input (mapped as a texture) to pick up the surrounding environment. The result is an accelerometer-navigable space which dynamically changes according to the real-world objects and artefacts the user focuses on.                   
                    </p>
                      <img alt="" class="bannerimg" src="img/iconAndroidCam.png"/>

                    <h2>
                        SeaOfSelFIES////////UNITY3D
                    </h2>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/JBstCgKlWZ8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p>
                        As a continuation from the previous experiment, I wanted to change the focus of the interaction from the environment to the tablet user. Build using Unity 3D, Sea of Selfies is a fun commentary on ‘selfie culture’ and the disposable commodity these images have become.                 
                    </p>
                    
                
                </div>
            </div>



            <footer>
                © Stephen Curtis 2018
            </footer>

        </div>
    </body>

</html>
